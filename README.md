
# Banking System Data Pipeline

This project simulates a streaming + batch data pipeline for a banking system using Kafka, PySpark, and MySQL. It handles transaction validation, customer and card updates, and credit score adjustments based on usage patterns.

## ğŸ“ Project Structure

```
.
â”œâ”€â”€ dataset_16/
â”‚   â”œâ”€â”€ customers.csv
â”‚   â”œâ”€â”€ cards.csv
â”‚   â”œâ”€â”€ transactions.csv
â”‚   â””â”€â”€ credit_card_types.csv
â”œâ”€â”€ results/
â”‚   â”œâ”€â”€ stream_transactions.csv
â”‚   â”œâ”€â”€ batch_transactions.csv
â”‚   â”œâ”€â”€ cards_updated.csv
â”‚   â””â”€â”€ customers_updated.csv
â”œâ”€â”€ .env
â”œâ”€â”€ main_stream.py
â”œâ”€â”€ main_batch.py
â”œâ”€â”€ main_kafka_producer.py
â”œâ”€â”€ main_mysql_loader.py
â”œâ”€â”€ main_mysql_updater.py
â””â”€â”€ README.md
```

## âš™ï¸ Technologies Used

- **Python**
- **Apache Kafka**
- **Apache Spark (PySpark)**
- **MySQL**
- **dotenv**
- **csv/json**

## ğŸ”„ Workflow

1. **Kafka Producer** - Reads CSV transactions and sends them to Kafka.
2. **Stream Processing** - Kafka consumer validates each transaction in real-time and updates the database.
3. **Batch Processing** - Approves pending transactions, updates balances, adjusts credit scores and limits.
4. **MySQL Updates** - Final Spark-based updates pushed back to the database.

## âœ… Transaction Validations

- Reject negative amounts (unless refunds).
- Reject large amounts (â‰¥ 50% of credit limit).
- Decline transactions with far merchant locations (based on ZIP code).
- Check if transaction exceeds credit limit with pending balance.

## ğŸ“¦ Installation

```bash
pip install -r requirements.txt
```

## ğŸ§ª Running the Pipeline

1. Set up `.env` with your MySQL and Kafka configs.
2. Load initial dataset:

```bash
python main_mysql_loader.py
```

3. Start Kafka and Zookeeper.
4. Produce transactions to Kafka:

```bash
python main_kafka_producer.py
```

5. Consume and validate transactions:

```bash
python main_stream.py
```

6. Perform batch updates:

```bash
python main_batch.py
```

7. Apply updates using Spark:

```bash
python main_mysql_updater.py
```

## ğŸ“ Outputs

- `results/stream_transactions.csv` - Transactions after stream validation.
- `results/batch_transactions.csv` - Transactions after batch approval.
- `results/cards_updated.csv` - Updated card balances and limits.
- `results/customers_updated.csv` - Updated customer credit scores.

## ğŸ‘¨â€ğŸ’» Author

Atharva Patil
Data Science Graduate Student @ RIT
